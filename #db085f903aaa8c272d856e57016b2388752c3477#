# Generated by the corresponding notebook.

from fastai.vision.all import *
import torch.nn as nn
import torch.nn.functional as F

# Data Preparation
path = untar_data(URLs.MNIST)

def flatten(x): 
    return x.view(x.size(0), -1)

dls = ImageDataLoaders.from_folder(path, train='training', valid='testing', )

# Model Definition
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.layers(x)

# Loss and Optimizer
device = torch.device("cuda:0")
model = MyModel().to(device)
opt = SGD(model.parameters(), lr=0.01)

# Training Loop
epochs = 10  # Number of epochs
for epoch in range(epochs):
    model.train()  # Set the model to training mode
    for xb, yb in dls.train:
        preds = model(xb)
        loss = F.cross_entropy(preds, yb)
        loss.backward()
        opt.step()
        opt.zero_grad()

    # Validation Loop
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():  # No gradient calculation needed
        val_loss = sum(F.cross_entropy(model(xb), yb) for xb, yb in dls.valid)
        val_loss /= len(dls.valid)
        
    # Evaluate training and validation accuracy
    def batch_accuracy(xb, yb):
        preds = xb.argmax(dim=1)
        return (preds == yb).float().mean()
    
    train_acc = sum(batch_accuracy(model(xb), yb) for xb, yb in dls.train) / len(dls.train)
    valid_acc = sum(batch_accuracy(model(xb), yb) for xb, yb in dls.valid) / len(dls.valid)
    
    print(f"Epoch: {epoch}, Training Loss: {loss}, Validation Loss: {val_loss}, Training Acc: {train_acc}, Validation Acc: {valid_acc}")